#goal is to see what predictors influence colour, and how accurate predictions are
> library(randomForest)
> library(ggplot2)
> #two ways to reduce variance in your model:
> #1. idea bagging: use boostrapping to sample hundreds of training sets and calculate a tree model for each set then average the models
> #2. idea random forests: same as bagging but only a limited number of predictors are used to calculate a give split
> set.seed(123) #makes random reproducable
> attach(diamonds)
> str(diamonds) #over 50,000 observations on 10 variables
Classes ‘tbl_df’, ‘tbl’ and 'data.frame':	53940 obs. of  10 variables:
 $ carat  : num  0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...
 $ cut    : Ord.factor w/ 5 levels "Fair"<"Good"<..: 5 4 2 4 2 3 3 3 1 3 ...
 $ color  : Ord.factor w/ 7 levels "D"<"E"<"F"<"G"<..: 2 2 2 6 7 7 6 5 2 5 ...
 $ clarity: Ord.factor w/ 8 levels "I1"<"SI2"<"SI1"<..: 2 3 5 4 2 6 7 3 4 5 ...
 $ depth  : num  61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...
 $ table  : num  55 61 65 58 58 57 57 55 61 61 ...
 $ price  : int  326 326 327 334 335 336 336 337 337 338 ...
 $ x      : num  3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...
 $ y      : num  3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...
 $ z      : num  2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...
 #we will use a subset of this large data set
> bagging<-randomForest(formula=color~.,data=diamonds[1:500,],mtr=9) #mtr determines the number for predictor variables use
> plot(bagging) #outputs error rates
> randomfor<-randomForest(formula=color~.,data=diamonds[1:500,],mtr=3) #three predictors per split
> importance(randomfor) #higher Gini values means the predictor has more influence
        MeanDecreaseGini
carat           43.20613
cut             22.29934
clarity         41.76484
depth           46.85141
table           35.30757
price           68.05994
x               51.57401
y               55.45062
z               49.37067
> varImpPlot(randomfor) #plots predictors vs Gini values. Price has highest value, then x,y,z to explain colour.
> #now lets check the accuracy of the models
> test<-diamonds[501:800,] #take observations not used to create the model
> predicted.bagging<-predict(newdata=test,bagging,type="class") #how does the bagging with new data fit?
> predicted.randomfor<-predict(newdata=test,randomfor,type="class") #how does the random forest with new data fit?
> table(predicted.bagging,test$color)
                 
predicted.bagging  D  E  F  G  H  I  J
                D 14 12  4  1  2  0  1
                E 17 49 46 16  4  2  1
                F  4 12 14  7  2  2  1
                G  1  3  1  3  1  5  0
                H  2  0  8  8 17 10  1
                I  0  0  1  1  5  4  1
                J  0  0  0  1  4  4  8
#values along the diagonal are correct predictions, off-diagonals are incorrect predictions
> table(predicted.randomfor,test$color)
                   
predicted.randomfor  D  E  F  G  H  I  J
                  D  7  7  2  1  0  0  1
                  E 23 49 45 14  6  0  0
                  F  5 13 14  6  1  1  0
                  G  1  2  2  2  2  5  0
                  H  2  0  8  9 17 10  1
                  I  0  5  3  4  4  7  3
                  J  0  0  0  1  5  4  8
> sum(diag(table(predicted.bagging,test$color)))/300 #gives the percentage of the bagging model giving the right colour
[1] 0.3633333
> sum(diag(table(predicted.randomfor,test$color)))/300 #gives the percentage of the random forest model giving the right colour
[1] 0.3466667
> #note that 0.35 is better than the 1/7 that would be expected due to random chance, though not a very high prediction rate
